{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c1f162-dd5a-40b6-b582-55979fa8f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Normalizer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner, NGramGenerator, PerceptronModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a6a255-f6fb-4ce9-903d-ba79ee15fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"gs://msca-bdp-student-gcs/Group7_Final_Project/airline_reviews/airline_reviews_preprocessed.csv\", \n",
    "                                     header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018ec08c-e412-41c5-b5ae-d59845fcaf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121296"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e2e9b62-82f1-4ddb-9235-0fa878436223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.where('overall_score is not null')\n",
    "df3 = df2.where('review is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f6d1f6-f877-4836-839b-4bf9bcdcef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a112b65c-d96e-498c-81a3-cc222ed4ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>count</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>48942</td>\n",
       "      <td>0.421387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>67203</td>\n",
       "      <td>0.578613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  count       pct\n",
       "0  Positive  48942  0.421387\n",
       "1  Negative  67203  0.578613"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df3.withColumn('sentiment', lit(None))\n",
    "df3 = df3.withColumn('sentiment', when((col('overall_score') >= 1) & (col('overall_score') <= 5), \"Negative\").otherwise(col('sentiment')))\n",
    "df3 = df3.withColumn('sentiment', when((col('overall_score') >= 6) & (col('overall_score') <= 10), \"Positive\").otherwise(col('sentiment')))\n",
    "sentiment_dist = df3.groupBy('sentiment').count()\n",
    "sentiment_dist = sentiment_dist.withColumn('pct', col('count')/116145)\n",
    "sentiment_dist.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5810d051-c8d6-435e-b9f9-bc55baf4f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The customer service rep I spoke to was incred...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last year my family and I took a trip to NY on...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JFK-RDU: First time flying this carrier. They ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All round good airline and nice flight. The se...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JFK-FLL roundtrip. FLL bound on E190. Spacious...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116140</th>\n",
       "      <td>Treviso to Lviv. Seemed like a new plane. Very...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116141</th>\n",
       "      <td>Rome to Prague. Was very happy with the flight...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116142</th>\n",
       "      <td>We often fly with Wizzair to/from Charleroi/Bu...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116143</th>\n",
       "      <td>PRG-LTN and LTN-PRG were rather good flights. ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116144</th>\n",
       "      <td>London - Kiev. First problem started a few wee...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116145 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "0       The customer service rep I spoke to was incred...  Negative\n",
       "1       Last year my family and I took a trip to NY on...  Positive\n",
       "2       JFK-RDU: First time flying this carrier. They ...  Positive\n",
       "3       All round good airline and nice flight. The se...  Positive\n",
       "4       JFK-FLL roundtrip. FLL bound on E190. Spacious...  Positive\n",
       "...                                                   ...       ...\n",
       "116140  Treviso to Lviv. Seemed like a new plane. Very...  Positive\n",
       "116141  Rome to Prague. Was very happy with the flight...  Positive\n",
       "116142  We often fly with Wizzair to/from Charleroi/Bu...  Positive\n",
       "116143  PRG-LTN and LTN-PRG were rather good flights. ...  Negative\n",
       "116144  London - Kiev. First problem started a few wee...  Negative\n",
       "\n",
       "[116145 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[['review', 'sentiment']].toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722a5ee8-de69-463a-ae72-02d16aff1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopwords = [\n",
    "\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ad017d-c329-4f31-9c5c-5ef0c1399f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol = \"review\", outputCol = \"words\", pattern = \"\\\\W\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol = \"words\", outputCol = \"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "pipeline1 = Pipeline(stages = [regexTokenizer, stopwordsRemover])\n",
    "\n",
    "pipelineFit1 = pipeline1.fit(df3)\n",
    "dataset1 = pipelineFit1.transform(df3)\n",
    "\n",
    "countVectors = CountVectorizer(inputCol = \"filtered\", outputCol = \"features\", vocabSize = 30000, minDF = 5)\n",
    "count_vector_model = countVectors.fit(dataset1)\n",
    "dataset1_2 = count_vector_model.transform(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa64dcc-9cb4-450d-a6d1-9bbe0c3aebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lda = LDA(k = 10, seed = 123, optimizer = \"em\", featuresCol = \"features\")\n",
    "ldamodel = lda.fit(dataset1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c8fea5-1702-4aa2-bbdd-b598dd12bc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 825:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                                                                   topicWords|\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    0| [flight, service, time, airline, good, staff, food, seats, flights, seat, hours, check, airport, plane, crew, airlines, fly, 2, cabin, told]|\n",
      "|    1| [flight, service, time, airline, good, food, staff, seats, seat, flights, hours, plane, airport, crew, check, airlines, fly, cabin, 2, told]|\n",
      "|    2|[flight, service, time, airline, good, food, seats, seat, staff, crew, flights, check, plane, hours, airport, airlines, cabin, class, fly, 2]|\n",
      "|    3| [flight, service, time, airline, good, food, staff, seat, seats, flights, hours, crew, plane, airport, check, airlines, fly, cabin, 2, told]|\n",
      "|    4|[flight, service, time, airline, good, food, seat, seats, staff, flights, crew, check, plane, hours, airlines, airport, cabin, class, fly, 2]|\n",
      "|    5|[flight, service, time, airline, good, food, seats, seat, staff, flights, crew, hours, plane, check, airport, airlines, cabin, fly, class, 2]|\n",
      "|    6|[flight, service, time, airline, good, food, staff, seats, seat, flights, hours, crew, plane, airport, check, airlines, cabin, fly, 2, class]|\n",
      "|    7|[flight, service, time, airline, good, food, seats, seat, staff, flights, crew, hours, plane, airport, check, airlines, fly, cabin, class, 2]|\n",
      "|    8| [flight, service, time, airline, good, food, staff, seats, flights, seat, hours, plane, airport, crew, check, airlines, fly, cabin, 2, told]|\n",
      "|    9|[flight, service, time, airline, good, food, seats, staff, seat, flights, crew, hours, plane, airport, check, airlines, cabin, fly, class, 2]|\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vocab = count_vector_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = udf(get_words, ArrayType(StringType()))\n",
    "\n",
    "num_top_words = 20\n",
    "\n",
    "topics = ldamodel.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(col('termIndices')))\n",
    "\n",
    "topics.select('topic', 'topicWords').show(truncate = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c077fa-93ef-491f-b086-40eb3d5c2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[ | ]pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('review') \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemmatized')\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(add_stopwords)\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemmatized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')\n",
    "\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "    .setInputCols(['document', 'lemmatized']) \\\n",
    "    .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a3749ec-990e-4951-b5bd-1a82e03db5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef95a32-7c8f-4a1e-aeca-b9b99d1ad636",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,                  \n",
    "                 tokenizer,\n",
    "                 normalizer,                  \n",
    "                 lemmatizer,                  \n",
    "                 stopwords_cleaner, \n",
    "                 pos_tagger,\n",
    "                 ngrammer,  \n",
    "                 finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eee076e-a259-4e22-a5c6-0fe2b66c3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pipeline.fit(df3).transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "496dafc7-ab27-419e-8408-4973b4564d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aircraft: string (nullable = true)\n",
      " |-- airline_name: string (nullable = true)\n",
      " |-- cabin_type: string (nullable = true)\n",
      " |-- date_flown: string (nullable = true)\n",
      " |-- date_pub: string (nullable = true)\n",
      " |-- entertainment_rating: integer (nullable = true)\n",
      " |-- food_rating: integer (nullable = true)\n",
      " |-- ground_service_rating: integer (nullable = true)\n",
      " |-- origin_country: string (nullable = true)\n",
      " |-- overall_score: integer (nullable = true)\n",
      " |-- recommended: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      " |-- route: string (nullable = true)\n",
      " |-- seat_comfort_rating: integer (nullable = true)\n",
      " |-- service_rating: integer (nullable = true)\n",
      " |-- slug: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- travel_type: string (nullable = true)\n",
      " |-- trip_verified: string (nullable = true)\n",
      " |-- value_rating: integer (nullable = true)\n",
      " |-- wifi_rating: integer (nullable = true)\n",
      " |-- unique_id: string (nullable = true)\n",
      " |-- day_pub: string (nullable = true)\n",
      " |-- month_pub: string (nullable = true)\n",
      " |-- year_pub: integer (nullable = true)\n",
      " |-- month_flown: string (nullable = true)\n",
      " |-- year_flown: integer (nullable = true)\n",
      " |-- day_pub_2: string (nullable = true)\n",
      " |-- month_pub_2: integer (nullable = true)\n",
      " |-- month_flown_2: integer (nullable = true)\n",
      " |-- date_pub_2: string (nullable = true)\n",
      " |-- date_flown_2: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- finished_unigrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- finished_ngrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- finished_pos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7fcd2c6-8814-45a5-a713-e5c7764d1d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 833:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|              review|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|The customer serv...|[customer, servic...|[the, customer, s...|[DT, NN, NN, NN, ...|\n",
      "|Last year my fami...|[year, family, tr...|[last, year, i, f...|[JJ, NN, NNP, NN,...|\n",
      "|JFK-RDU: First ti...|[jfkrdu, time, fl...|[jfkrdu, first, t...|[NN, JJ, NN, NN, ...|\n",
      "|All round good ai...|[round, good, air...|[all, round, good...|[DT, NN, JJ, NN, ...|\n",
      "|JFK-FLL roundtrip...|[jfkfll, roundtri...|[jfkfll, roundtri...|[NN, NN, NN, NN, ...|\n",
      "|ORD-BOS-SJU retur...|[ordbossju, retur...|[ordbossju, retur...|[NN, NN, NN, NN, ...|\n",
      "|BWI-BOS-BWI on E1...|[bwibosbwi, servi...|[bwibosbwi, on, e...|[NN, IN, SYM, CC,...|\n",
      "|Harassed by rude ...|[harass, rude, fl...|[harass, by, rude...|[NN, IN, NN, NN, ...|\n",
      "|On November 24th ...|[november, arrive...|[on, november, th...|[IN, NNP, NN, NNP...|\n",
      "|We are a military...|[military, family...|[we, be, a, milit...|[PRP, VB, DT, JJ,...|\n",
      "|Using Covid as an...|[covid, excuse, p...|[use, covid, as, ...|[NN, NN, IN, DT, ...|\n",
      "|I just wanted to ...|[compliment, serv...|[i, just, want, t...|[NNP, RB, VBP, TO...|\n",
      "|Round trip from O...|[round, trip, orl...|[round, trip, fro...|[NN, NN, IN, NN, ...|\n",
      "|Cabin crew friend...|[cabin, crew, fri...|[cabin, crew, fri...|[NN, NN, RB, CC, ...|\n",
      "|\"Non-stop red-eye...|[nonstop, redeye,...|[nonstop, redeye,...|[NN, NN, NN, DT, ...|\n",
      "|NY to Ft. Lauderd...|[lauderdale, minu...|[ny, to, ft, laud...|[NN, TO, NN, NN, ...|\n",
      "|OAK-IAD. Best fli...|[oakiad, good, fl...|[oakiad, good, fl...|[NN, JJ, NN, NN, ...|\n",
      "|Roundtrip Oakland...|[roundtrip, oakla...|[roundtrip, oakla...|[NN, NN, TO, JJ, ...|\n",
      "|Loved flying Delt...|[love, fly, delta...|[love, fly, delta...|[NN, NN, NN, NN, ...|\n",
      "|I don't know how ...|[dont, ill, start...|[i, dont, know, h...|[NNP, NN, VB, WRB...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df4[['review', 'finished_unigrams', 'finished_ngrams', 'finished_pos']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ddc111-48c8-447d-a8c7-e2289ca8bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ed49a1c-3e9a-446e-8ea8-65b077b565ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "udf_join_arr = udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(col('finished_pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09d336c5-d74f-4fbb-a4b5-1340d2f680db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da34bdb2-8ba3-40b6-8cf4-4640e03bfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a375e2a-9435-46fa-8550-fe3e027e5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a4b496-861e-4074-af52-a420d18a91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fad9434-a0b8-45b2-9f28-75b9f6394e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3726874d-25af-4547-a517-8287911750b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6a1c57a-2a19-42cc-ac04-be0193c2c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aircraft: string (nullable = true)\n",
      " |-- airline_name: string (nullable = true)\n",
      " |-- cabin_type: string (nullable = true)\n",
      " |-- date_flown: string (nullable = true)\n",
      " |-- date_pub: string (nullable = true)\n",
      " |-- entertainment_rating: integer (nullable = true)\n",
      " |-- food_rating: integer (nullable = true)\n",
      " |-- ground_service_rating: integer (nullable = true)\n",
      " |-- origin_country: string (nullable = true)\n",
      " |-- overall_score: integer (nullable = true)\n",
      " |-- recommended: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      " |-- route: string (nullable = true)\n",
      " |-- seat_comfort_rating: integer (nullable = true)\n",
      " |-- service_rating: integer (nullable = true)\n",
      " |-- slug: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- travel_type: string (nullable = true)\n",
      " |-- trip_verified: string (nullable = true)\n",
      " |-- value_rating: integer (nullable = true)\n",
      " |-- wifi_rating: integer (nullable = true)\n",
      " |-- unique_id: string (nullable = true)\n",
      " |-- day_pub: string (nullable = true)\n",
      " |-- month_pub: string (nullable = true)\n",
      " |-- year_pub: integer (nullable = true)\n",
      " |-- month_flown: string (nullable = true)\n",
      " |-- year_flown: integer (nullable = true)\n",
      " |-- day_pub_2: string (nullable = true)\n",
      " |-- month_pub_2: integer (nullable = true)\n",
      " |-- month_flown_2: integer (nullable = true)\n",
      " |-- date_pub_2: string (nullable = true)\n",
      " |-- date_flown_2: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- finished_unigrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- finished_ngrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- finished_pos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- finished_pos_ngrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de2542ee-18b7-44af-b388-6b11819862f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 834:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     finished_ngrams| finished_pos_ngrams|\n",
      "+--------------------+--------------------+\n",
      "|[the, customer, s...|[DT, NN, NN, NN, ...|\n",
      "|[last, year, i, f...|[JJ, NN, NNP, NN,...|\n",
      "|[jfkrdu, first, t...|[NN, JJ, NN, NN, ...|\n",
      "|[all, round, good...|[DT, NN, JJ, NN, ...|\n",
      "|[jfkfll, roundtri...|[NN, NN, NN, NN, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0c753d8-34d7-4fb3-9ca4-13d8812c9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
    "\n",
    "udf_filter_pos = udf(filter_pos, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d6b20ce-1ac4-4d6c-9cbb-853dbcb1566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_unigrams',\n",
    "                                               udf_filter_pos(col('finished_unigrams'), \n",
    "                                                              col('finished_pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de04efa8-43a1-4a46-ae72-1b50d892bba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 837:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                         filtered_unigrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[service, rep, speak, rude, employee, instantly, cancellation, website, middle, speak, ...|\n",
      "|[year, family, jet, impressed, experience, entertainment, happy, busy, extremely, frien...|\n",
      "|[jfkrdu, time, fly, carrier, terminal, security, time, min, friendly, situation, fine, ...|\n",
      "|[good, airline, nice, service, great, colombia, orlando, orlando, lovely, great, happy,...|\n",
      "|[jfkfll, roundtrip, fll, bind, shoulder, room, channel, friendly, helpful, return, flig...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_unigrams').limit(5).show(truncate = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1888a6c2-f15f-4c49-9d28-122c3ea1573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_combs(words, pos_tags):\n",
    "    return [word for word, pos in zip(words, pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_combs = udf(filter_pos_combs, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3acc37ea-9e4d-429d-adf2-4ffc0aac0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_review = processed_review.withColumn('filtered_ngrams',\n",
    "                                               udf_filter_pos_combs(col('finished_ngrams'),\n",
    "                                                                    col('finished_pos_ngrams')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43c466fe-feae-48eb-a2d4-373bb424f65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 840:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                           filtered_ngrams|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[customer_service, service_rep, be_terminate, didnt_understand, delta_website, be_utter...|\n",
      "|[last_year, jet_blue, first_time, onboard_entertainment, child_happy, flight_crew, be_s...|\n",
      "|[jfkrdu_first, first_time, time_fly, nice_terminal, security_line, line_take, long_time...|\n",
      "|[round_good, good_airline, nice_flight, be_great, bogota_colombia, orlando_crew, great_...|\n",
      "|[jfkfll_roundtrip, roundtrip_fll, fll_bind, spacious_leg, shoulder_room, room_direct, d...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_review.select('filtered_ngrams').limit(5).show(truncate = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b28878f-398d-4df5-95f7-4e99f1f28645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "processed_review = processed_review.withColumn('final', \n",
    "                                               concat(col('filtered_unigrams'), \n",
    "                                                      col('filtered_ngrams')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b828cc21-3e17-49a2-b9a2-cded729a2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tfizer = CountVectorizer(inputCol = 'final', outputCol = 'tf_features')\n",
    "tf_model = tfizer.fit(processed_review)\n",
    "tf_result = tf_model.transform(processed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdd1ceda-8866-4744-908a-87b6af835108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/24 05:04:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idfizer = IDF(inputCol = 'tf_features', outputCol = 'tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7f929a4-f5e2-4192-b78e-66cc79982038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/24 06:18:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "23/11/24 06:25:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "23/11/24 06:25:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:25:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:25:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:25:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:25:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:25:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:25:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:25:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:25:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:25:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:25:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:25:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "23/11/24 06:26:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "23/11/24 06:26:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 28.4 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_topics = 5\n",
    "max_iter = 25\n",
    "\n",
    "lda = LDA(k = num_topics, maxIter = max_iter, featuresCol = 'tf_idf_features')\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4565df73-d29c-48d6-a1fc-6e2663c213bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                                                                            topicWords|\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    0|[hour, delay, flight, airline, customer_service, airport, time, bag, customer, day, check, fly, wait, plane, be_delay, pay, board, cancel, gate, book]|\n",
      "|    1|[seat, business_class, class, good, food, economy, service, business, flight, cabin, fly, crew, time, staff, entertainment, airline, meal, be_good,...|\n",
      "|    2|[flight, airline, customer, customer_service, air, book, service, refund, time, ticket, fly, hour, pay, day, seat, travel, cancel, bad, check, airp...|\n",
      "|    3|[business_class, seat, business, class, good, cabin, crew, service, food, cabin_crew, flight, staff, qatar_airway, meal, return, excellent, qatar, ...|\n",
      "|    4|[crew, good, cabin_crew, meal, cabin, seat, drink, food, flight, service, time, serve, nice, board, comfortable, friendly, great, fly, entertainmen...|\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = udf(get_words, ArrayType(StringType()))\n",
    "\n",
    "num_top_words = 20\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(col('termIndices')))\n",
    "\n",
    "topics.select('topic', 'topicWords').show(truncate = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c9bab-735a-4a17-8c87-c5dd83461107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
